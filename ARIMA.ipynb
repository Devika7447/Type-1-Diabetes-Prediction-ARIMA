{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Z4Iwiqv9Up1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Specify the semicolon as the delimiter\n",
        "        temp_data = pd.read_csv(file_path, sep=';')\n",
        "        all_data = pd.concat([all_data, temp_data], ignore_index=True)\n",
        "\n",
        "# Display the combined dataset\n",
        "print(\"Combined DataFrame:\")\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "iQEJv2JRUqvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one of the CSV files for inspection\n",
        "file_path = '/content/drive/My Drive/Preprocessed/HUPA0015P.csv'  # Replace with the actual path\n",
        "data = pd.read_csv(file_path, sep=';', header=None)  # Use sep=';' to handle semicolon delimiter\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "k682smNfUq7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path, sep=';', header=0, engine='python')  # Use 'python' engine for better compatibility\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "jgQ1ZeF9UrG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read and split the data\n",
        "        temp_data = pd.read_csv(file_path, header=None)\n",
        "        temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "        temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "        # Combine into the main DataFrame\n",
        "        all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "JwJdKbliU1oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "# Handle missing values\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'  # Replace with your actual target column name\n",
        "X = all_data.drop(columns=[target_column, 'time'])  # Drop 'time' column from features\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, errors='coerce' to handle potential non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Hg3jEMX3U3nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaWIowNrUOvQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "# Convert 'glucose' to numeric before dropping NaNs\n",
        "all_data['glucose'] = pd.to_numeric(all_data['glucose'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in 'glucose' column\n",
        "all_data = all_data.dropna(subset=['glucose'])\n",
        "\n",
        "# Extract glucose data for ARIMA\n",
        "glucose_data = all_data['glucose'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data = train_test_split(glucose_data, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Fit ARIMA model\n",
        "# Adjust order parameters (p, d, q) based on your data's characteristics\n",
        "# You might need to analyze ACF and PACF plots to determine optimal values\n",
        "model = ARIMA(train_data, order=(5, 1, 0))\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(glucose_data) - 1)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "print(\"ARIMA Mean Squared Error:\", mse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ... (Data loading, preprocessing, and ARIMA model fitting as before) ...\n",
        "\n",
        "# Make predictions\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(glucose_data) - 1)\n",
        "\n",
        "# --- Regression Metrics ---\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "mae = mean_absolute_error(test_data, predictions)\n",
        "rmse = np.sqrt(mse)  # Calculate RMSE\n",
        "r2 = r2_score(test_data, predictions)\n",
        "\n",
        "print(\"\\nARIMA Regression Metrics:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# --- Categorization for Classification Metrics ---\n",
        "thresholds = [70, 140]  # Example thresholds (adjust as needed)\n",
        "y_test_cat = np.digitize(test_data, thresholds)  # Using test_data for true values\n",
        "predictions_cat = np.digitize(predictions, thresholds)\n",
        "\n",
        "# --- Classification Metrics ---\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "cm = confusion_matrix(y_test_cat, predictions_cat)\n",
        "precision = precision_score(y_test_cat, predictions_cat, average='weighted')\n",
        "recall = recall_score(y_test_cat, predictions_cat, average='weighted')\n",
        "accuracy = accuracy_score(y_test_cat, predictions_cat)\n",
        "f1 = f1_score(y_test_cat, predictions_cat, average='weighted')\n",
        "\n",
        "print(\"\\nARIMA Classification Metrics:\")\n",
        "print(\"Confusion Matrix:\", cm)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# ... (Further analysis and visualization) ..."
      ],
      "metadata": {
        "id": "kddR25SoUonB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}